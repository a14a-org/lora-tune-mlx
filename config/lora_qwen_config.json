{
    "model_name": "mlx-community/Qwen2.5-7B-Instruct-bf16",
    "lora_config": {
        "r": 16,
        "alpha": 32,
        "dropout": 0.1,
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
        "task_type": "CAUSAL_LM"
    },
    "training_config": {
        "num_epochs": 3,
        "max_steps": 2000,
        "learning_rate": 3e-4,
        "warmup_steps": 100,
        "batch_size": 4,
        "gradient_accumulation_steps": 4,
        "weight_decay": 0.001,
        "max_grad_norm": 0.3,
        "logging_steps": 10,
        "save_steps": 100,
        "eval_steps": 50,
        "output_dir": "checkpoints/lora_qwen_v3_1k"
    },
    "data_config": {
        "train_file": "data/qwen_format_v3/train.jsonl",
        "validation_file": "data/qwen_format_v3/valid.jsonl",
        "max_seq_length": 2048,
        "preprocessing_num_workers": 4
    }
} 